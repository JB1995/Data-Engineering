{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c77e48-acd8-4481-8b33-a3afe802d33d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## # Reading files, dataframes and parsing json data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47d5edd-f8cc-46ed-9664-bc5bd113d833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Note: Creating dataframe with single field\n",
    "> data = [(\"Alice\",)] or data = [[\"Alice\"]]\n",
    "\n",
    "If we do `data = [(\"Alice\")]`, it is considered as `data = [\"Alice\"]`, which is a list of strings, not a list of tuples or lists. To create a DataFrame with a single column, use `data = [(\"Alice\",)]` (list of tuples) or `data = [[\"Alice\"]]` (list of lists).\n",
    "\n",
    "For creating DataFrames in PySpark, tuples offer better performance and memory efficiency compared to lists due to their immutable nature. Unless you need to modify the data before creating the DataFrame, tuples are generally the preferred choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a428605-cc46-4e15-ad8f-0c32bd6fc3bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, types as T, functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, DoubleType, LongType\n",
    "from pyspark.sql.window import Window as W\n",
    "spark=SparkSession.builder.appName('Spark').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85ebb8ad-1581-4258-b35b-86b2b92cbd84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create DataFrame & Json field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "073fa67d-edb9-468f-848d-7548e5dbd5fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[('Biki',29), ('Raveen', 25)]\n",
    "fields=StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('age', IntegerType(), True)\n",
    "])\n",
    "df=spark.createDataFrame(data=data, schema=fields)\n",
    "# create json data column\n",
    "df=df.withColumn('json_data', F.to_json(F.struct(*df.columns)))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3faeec30-777d-4029-86c2-91107d56e915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## List files from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ce53fb1-c6ca-4bd6-85ad-3770eb867938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filepath='/Volumes/workspace/biki/files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6de81a-88ea-42c2-a5e0-e2cca1def666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Approach 1: To list files using databricks dbutils\n",
    "files=dbutils.fs.ls(filepath)\n",
    "print(files)\n",
    "print(\"=========================\")\n",
    "f=[]\n",
    "for k in files:\n",
    "    f.append({k.name:k.path})\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f34719c-6b2c-4e43-85fc-7bca0f239bac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Approach 2: To list files using python library\n",
    "import os\n",
    "files = os.listdir(filepath)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46746a19-b066-4340-8d3f-b72175378f07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reading different file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59eb174b-67c2-427f-9441-61160b49ea02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csvoptions={'header':True, 'inferSchema': True}\n",
    "csv_schema=StructType([\n",
    "  StructField('employee_id', IntegerType(), True)\n",
    "  , StructField('first_name', StringType(), True)\n",
    "  , StructField('last_name', StringType(), True)\n",
    "  , StructField('department', StringType(), True)\n",
    "  , StructField('salary', DoubleType(), True)\n",
    "  , StructField('join_date', DateType(), True)\n",
    "])\n",
    "json_schema=StructType([\n",
    "  StructField('City', StringType(), True),\n",
    "  StructField('State', StringType(), True),\n",
    "  StructField('Zip', StructType([\n",
    "    StructField('ZipCodeType', StringType(), True),\n",
    "    StructField('Zipcode', LongType(), True)\n",
    "  ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88ee912b-c6d8-4036-bd4f-ffcd1e1404a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for f in files:\n",
    "  ext=f.split('.')[-1]\n",
    "  if(ext=='csv'):\n",
    "    # csv_df=spark.read.format('csv').option('header', True).option('inferSchema', True).load(f'{filepath}{f}')\n",
    "    # csv_df=spark.read.format('csv').options(**csvoptions).load(f'{filepath}{f}')\n",
    "    csv_df=spark.read.format('csv').option('header', True).schema(csv_schema).load(f'{filepath}{f}')\n",
    "    csv_df.show(truncate=False)\n",
    "  elif(ext=='json'):\n",
    "    # json_df=spark.read.format('json').load(f'{filepath}{f}')\n",
    "    json_df=spark.read.format('json').schema(json_schema).load(f'{filepath}{f}')\n",
    "    json_df.show(truncate=False)\n",
    "  elif(ext=='txt'):\n",
    "    txt_df=spark.read.format('text').load(f'{filepath}{f}')\n",
    "    txt_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "915541f1-3f49-4b47-8150-563f83226446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analysing JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3378c649-c007-4bc4-bcff-769ab244fb40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_extract_df=json_df.select(F.col('*'), F.col('Zip.*'))\n",
    "json_extract_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "467db8e2-9f16-45f6-90af-a7d312bb13d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('select * from {df}', df=json_df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "643d705f-aa4f-4871-a2a4-90bc9418bdab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_df.createOrReplaceTempView('json_view')\n",
    "spark.sql('select * from json_view').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f78b0fd-daaf-4456-acc5-a4840a58246b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reading JSON data from a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fff45589-94c9-43fa-9e10-f27890b39cd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract fields from parsed json column\n",
    "txt_df_parsed = txt_df.withColumn('parsed_value', F.from_json('value',json_schema))\n",
    "txt_df_parsed.select('*', 'parsed_value.Zip.*', 'parsed_value.City', 'parsed_value.State').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e961d6-f61c-4233-8c87-fdabca22c7ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access json value column without parsing\n",
    "txt_df.select( '*', \n",
    "    F.get_json_object('value', '$.City').alias('City'),\n",
    "    F.get_json_object('value', '$.State').alias('State'),\n",
    "    F.get_json_object('value', '$.Zip.ZipCodeType').alias('ZipCodeType'),\n",
    "    F.get_json_object('value', '$.Zip.Zipcode').alias('Zipcode')\n",
    ")\\\n",
    ".show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Q4.Reading files and parsing json data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
